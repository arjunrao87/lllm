# Local LLM (lllm)

A series of experiments with local LLMs

# TODOs

## Exp (Using Ollama)

- [x] Run mistral 7b locally
- [x] Run llama 2 locally
- [x] Run custom prompts with llama 2
- [x] Run API calls against llama/mistral

## Exp (Get Twinny to work in VSCode)

- [x] Done with caveats

## Exp (Using Ollama for a Chat UI)

- [x] Create chat UI similar to [this](https://www.youtube.com/watch?v=n9AMtXLveMs)

## Exp (Create your own version of an open source LLM)

- [x] Custom prompts + temps

## Exp (Train Mistral on your own data - finetuning or use RAG)
